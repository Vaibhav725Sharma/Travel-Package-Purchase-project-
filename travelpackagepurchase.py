# -*- coding: utf-8 -*-
"""TravelPackagePurchase.ipynb

Automatically generated by Colaboratory.

"""


import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from classification_algorithm import Classifier
import seaborn as sns
import itertools as it     #It is simply for converting n dimensional list to single dimension
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.model_selection import GridSearchCV
import warnings

warnings.filterwarnings('ignore')

 
df = pd.read_csv('E://vscodeprojects//tourism.csv')
df.head()

"""Steps i'll follow
1. handle missing values in numerical features and categorical features
2. encode the categorical variables
3. checks the distribution and treats the outlier
4. select the essential features i.e. feature selection
5. build classification algorithm

# 1. Handle missing values
"""

df.describe()    #brief about our dataset

df.isnull().sum()   #Age, TypeofContact, DurationOfPitch, NumberOfFollowups, PreferredPropertyStar, NumberOfTrips, NumberOfChildrenVisited, MonthlyIncome are the features which have missing values

num_feature = [col for col in df.columns if df[col].dtypes != 'O']     # numerical features :-  ['CustomerID','ProdTaken','Age','CityTier','DurationOfPitch','NumberOfPersonVisited','NumberOfFollowups','PreferredPropertyStar','NumberOfTrips','Passport','PitchSatisfactionScore','OwnCar','NumberOfChildrenVisited','MonthlyIncome']
cat_feature = [col for col in df.columns if df[col].dtypes == 'O']     # categorical features :- ['TypeofContact','Occupation','Gender','ProductPitched','MaritalStatus','Designation']
num_nan = [col for col in num_feature if df[col].isnull().any()]       # numerical features having missing values :-  ['Age','DurationOfPitch','NumberOfFollowups','PreferredPropertyStar','NumberOfTrips','NumberOfChildrenVisited','MonthlyIncome']
cat_nan = [col for col in cat_feature if df[col].isnull().any()]       # categorical features having missing values :- ['TypeofContact']

"""we'll remove the missing values by 3 ways
1. Mean/Median Replacement
2. Random Sample Imputation
3. End of Distribution Imputation

# Mean/Median Replacement

missing values are replaced with median of that feature in which value present
"""

for col in num_nan:
    df[col+'_median'] = df[col].fillna(df[col].median())

"""# Random Sample Imputation

missing values are replaced with some random sample values of that feature in which value present
"""

for col in num_nan:
    df[col+'_random'] = df[col]
    sample = df[col].dropna().sample(df[col].isnull().sum(),random_state = 0)
    sample.index = df[df[col].isnull()].index
    df.loc[df[col].isnull(),col+'_random'] = sample

"""# End Of Distribution Imputation

missing values are replaced the value which is present at the end of distribution of a feature gaussian distribution i.e. after 3 standard deviation
"""

for col in num_nan:
    val = df[col].mean() + 3*(df[col].std())
    df[col+'_end_dist'] = df[col].fillna(val)

# lets check the distribution of that new columns that we made

sns.kdeplot(df.Age)    # blue colored 
sns.kdeplot(df.Age_median)  #orange colored     Age_median has some change in variance
sns.kdeplot(df.Age_random)   #no change in variance at all
sns.kdeplot(df.Age_end_dist)   #quite a bit change in variance

"""As we can see in the above plot numerical features are imputed successfully with less change in distribution
so apply random sample imputation in our variable
and drop other features that we've created for testing
"""

for i in num_nan:
    df.drop(i+'_median',axis = 1,inplace = True)
    df.drop(i+'_random',axis = 1,inplace = True)
    df.drop(i+'_end_dist',axis = 1,inplace = True)

"""#As we can see below,extra columns are removed"""

df.columns

#lets apply random sample imputation in original features
for col in num_nan:
    sample = df[col].dropna().sample(df[col].isnull().sum(),random_state = 0)
    sample.index = df[df[col].isnull()].index
    df.loc[df[col].isnull(),col] = sample

"""#nan values are removed from numerical features"""

df[num_feature].isnull().sum()

#we can drop CustomerID as it is only for identification
df.drop('CustomerID',axis = 1,inplace = True)

"""#now lets handle missing values in categorical variables using most frequent count imputation"""

# We have only 1 categorical column which have nan value, we'll check the most occured label of that feature
for col in cat_nan:
    print(df[col].mode()[0])  #as self enquiry comes most so we can replace all nan with self enquiry

most_frequent = df['TypeofContact'].mode()[0]
most_frequent

df['TypeofContact'].fillna(most_frequent,inplace = True)

# nan values are also removed from categorical features
df[cat_nan].isnull().sum()

"""#We've handles all misssing values from our dataset"""

df.isnull().sum()

"""# 2. Encode the categorical variables"""

#lets check the labels in every feature
for col in cat_feature:
    print(f'{col} feature has {len(list(df[col].value_counts().index))} i.e.\n\n{list(df[col].value_counts().index)}\n')

"""# Lets encode the categorical features using get_dummies function

dummy_feature has all the categorical feature and it is replaced with 0 and 1
"""

dummy_feature = pd.get_dummies(df[cat_feature],drop_first = True)
dummy_feature.drop('Gender_Female',axis = 1,inplace = True)
dummy_feature.columns

#Now drop the original categorical feature and merge the dummy_feature with our dataset
df.drop(cat_feature,axis = 1,inplace = True)  #categorical feature are dropped
df = pd.concat([df,dummy_feature],axis = 1)   #concatenating the dummy features with our dataset

"""# Check the distribution of features"""

num_feature = num_feature[1:]
plt.figure(figsize = (10,70))
i=0
for feat in df.columns:
    i = i+1
    plt.subplot(len(df.columns),1,i)
    sns.boxplot(df[feat])

"""as we can see above, variables are not Gaussian distributed ,and boxplot is showing us the presence of outliers in almost every column

# 3. Now treat and remove the outlier
"""

#before removing outliers shape of dataframe is
df.shape

"""## We'll remove the outliers using iqr method
   25 % - Q1 (First Quantile)
   50 % - Q2 (Second Quantile)
   75 % - Q3 (Third Quantile)
   IQR(Inter Quantile Range) - Q3 - Q1
   lower_boundary = Q1 - 1.5*IQR
   upper_boundary = Q3 + 1.5*IQR

   values lesser than lower_boundary and greater than upper_boundary are considered as an outliers

   and in exceptional cases, we'll deal with that feature individually
"""


feature = df.drop('ProdTaken',axis = 1)  #excluding ProdTaken feature (dependent feature)
target = df['ProdTaken']
row_list = []
for col in feature:
    q1 = df[col].sort_values().quantile(0.25)
    q3 = df[col].sort_values().quantile(0.75)
    iqr = q3 - q1
    low_range = q1 - 1.5*iqr
    upr_range = q3 + 1.5*iqr
    row_list.append(list(df[(df[col] > upr_range) | (df[col] < low_range)].index))

"""row_list is a list of all the rows in which values are acting as an outlier.
So we'll remove the rows
"""

row_list = list(it.chain(*row_list))   #we get the list of all rows indices in which outliers are present
row_list = list(set(row_list))         #to remove duplicates rows

row_list #all the rows indices

"""Drop the rows"""

df.drop(row_list,inplace = True)

#after removing the outliers we get 2136 rows which is before 4888
df.shape

"""now lets check the distribution again"""

num_feature = num_feature[1:]
plt.figure(figsize = (7,100))
i=0
for feat in df.columns:
    i = i+1
    plt.subplot(len(df.columns),1,i)
    sns.boxplot(df[feat])

"""#As we can see Age and MonthlyIncome features has outliers remained
so lets focus more on these two features
"""

sns.boxplot(df.Age)    #values above 55(around) are also present as outliers so lets remove it

age_outlier = list(df[df.Age > 55].index)
df.drop(age_outlier,inplace = True)

sns.boxplot(df.Age)   #outliers are removed

sns.boxplot(df.MonthlyIncome)  #it has outliers having MonthlyIncome greater than 27550

income_outlier = list(df[df.MonthlyIncome > 27550].index)

df.drop(income_outlier,inplace = True)

sns.boxplot(df.MonthlyIncome)  #outliers are removed successfully

"""# all outliers are removed"""

#After removing all outliers, shape of our dataset is
df.shape

"""# Split the data in dependent and independent features and training and testing data"""

feature = df.drop('ProdTaken',axis = 1)   #Independent Features
target = df.ProdTaken                     #Dependent Features

"""#Feature Selection

Now let's select the 10 most essential features for our dataset using chi2 method
We'll consider top 10 features and remove other features
"""



ordered_rank_features=SelectKBest(score_func=chi2,k=20)
ordered_feature=ordered_rank_features.fit(feature,target)

scores=pd.DataFrame(ordered_feature.scores_,columns=["Score"])
column=pd.DataFrame(feature.columns)

features_rank=pd.concat([column,scores],axis=1)

features_rank.columns=['Features','Score']
features_rank

"""These all are important features that we should include in our dataset"""

imp_features = features_rank.nlargest(10,'Score')
feat_imp = list(imp_features.Features)

"""Update the feature dataset with top 10 essential features"""

feature = feature[feat_imp]

"""Now lets check the shape of feature and target"""

feature.shape,target.shape

"""# Train Test Splitting"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(feature,target,test_size = 0.2,random_state = 0,shuffle = False)

"""# Build a model

1. KNeighborsClassifier

2. SVC

3. RandomForestClassifier

4. AdaBoostClassifier

5. GaussianNB
"""



"""#lets build model on various classification (without hyperparameter tuning)"""




clf = Classifier(x_train, y_train, x_test, y_test)
clf.svm()
clf.adaboost()
clf.randomforest()
clf.xgboost()
clf.knn(3)
clf.gaussain_naiv_bayes()

"""#lets build model on various classifications (with hyperparameter tuning)

# 1. Random Forest Classification
"""

clf1 = RandomForestClassifier()
param_rfc = {'n_estimators':[100,200,250,300,350,400],'criterion' : ["gini", "entropy"],'max_depth':[5,6,7,4]}
rfc = GridSearchCV(clf1,param_rfc,scoring='precision')
rfc.fit(x_train,y_train)
print(rfc.best_params_)         #best parameters are :- {'criterion': 'entropy', 'max_depth': 7, 'n_estimators': 250}
print(rfc.best_score_)          #best score by applying best parameter is :- 0.8893650793650794

#accuracy_score(y_test,pred_rfc)

pred_rfc = rfc.predict(x_test)
confusion_matrix(y_test,pred_rfc)

"""# 2. Support Vector Classification"""

clf2 = SVC()
param_svc = {'C':[0.1,1,10,100],'kernel':['linear','poly','rbf','sigmoid']}
svc = GridSearchCV(clf2,param_svc,scoring = 'precision')
svc.fit(x_train,y_train)
print(svc.best_params_)         #best parameters are :- {'C': 0.1, 'kernel': 'linear'}
print(svc.best_score_)          #best score by applying best parameter is :- 0.4

#accuracy_score(y_test,pred_svc)

pred_svc = svc.predict(x_test)
confusion_matrix(y_test,pred_svc)

"""# 3. KNN Classification"""

clf3 = KNeighborsClassifier()
param_knn = {'n_neighbors':list(range(1,6)),'algorithm':['auto','ball_tree','kd_tree','brute']}
knn = GridSearchCV(clf3,param_knn,scoring = 'precision')
knn.fit(x_train,y_train)
print(knn.best_params_)      #best parameters are :- {'algorithm': 'auto', 'n_neighbors': 1}
print(knn.best_score_)       #best score is :- 0.5028759398496241 

#accuracy_score(y_test,pred_knn)

pred_knn = knn.predict(x_test)
confusion_matrix(y_test,pred_knn)

"""# 4. Gaussian Naive Bayes Classification"""

clf5 = GaussianNB()
param_nb = {'var_smoothing': np.logspace(0,-9, num=100)}
nb = GridSearchCV(clf5,param_nb,scoring = 'precision')
nb.fit(x_train,y_train)
print(nb.best_params_)         #best parameters are:- {'var_smoothing': 3.5111917342151277e-08}
print(nb.best_score_)          #best score is :- 0.7573856209150327

#accuracy_score(y_test,pred_nb)

pred_nb = nb.predict(x_test)
confusion_matrix(y_test,pred_nb)

"""# 5. AdaBoost Classification"""

clf4 = AdaBoostClassifier()
param_ada = {'n_estimators':list(range(50,101))}
ada = GridSearchCV(clf4,param_ada,scoring = 'precision')
ada.fit(x_train,y_train)
print(ada.best_params_)        #best parameters are :- {'n_estimators': 99}
print(ada.best_score_)         #best score is :- 0.5433766233766233

#accuracy_score(y_test,pred_ada)

pred_ada = ada.predict(x_test)
confusion_matrix(y_test,pred_ada)

"""#We will select the RandomForest Classification for our model


"""

import pickle
saved_model = pickle.dumps(rfc)
 
# Load the pickled model
Rfmodel = pickle.loads(saved_model)
 
# Use the loaded pickled model to make predictions
#knn_from_pickle.predict(X_test)

import joblib

joblib.dump(rfc, 'Rfmodel.pkl')

modified_df = pd.concat([feature,target],axis = 1)
modified_df.to_csv('modified_df.csv')




